{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d9a3772-9ccd-43a2-97e5-4526f9880caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "#Crated  by: Luisa Fernanda Buriticá Ruíz\n",
    "#Date: December 16, 2024\n",
    "#Contact: fernanda.buritica@udea.edu.co\n",
    "#Title: Automatic Data Download from IDEAM V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f825854-40a4-4c66-8c4b-32ade9ee1c79",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sodapy in c:\\users\\luisa f\\anaconda3\\envs\\coastsat\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: requests>=2.28.1 in c:\\users\\luisa f\\anaconda3\\envs\\coastsat\\lib\\site-packages (from sodapy) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\luisa f\\anaconda3\\envs\\coastsat\\lib\\site-packages (from requests>=2.28.1->sodapy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\luisa f\\anaconda3\\envs\\coastsat\\lib\\site-packages (from requests>=2.28.1->sodapy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\luisa f\\anaconda3\\envs\\coastsat\\lib\\site-packages (from requests>=2.28.1->sodapy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\luisa f\\anaconda3\\envs\\coastsat\\lib\\site-packages (from requests>=2.28.1->sodapy) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# 1. LIBRARIES\n",
    "# 1.1 Imported libraries (6)\n",
    "import os               # System interaction, file manipulation\n",
    "import pandas as pd     # Tabular data manipulation and analysis\n",
    "import time             # Time-related functions\n",
    "!pip install sodapy\n",
    "import config\n",
    "\n",
    "# 1.2 Imported modules (4)\n",
    "from requests.exceptions import ReadTimeout, ConnectionError, HTTPError, ChunkedEncodingError\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta  # Advanced date and time manipulation\n",
    "from sodapy import Socrata                # Interacting with Socrata API datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce5483f4-b0bd-41b0-9716-99ba8f6f798d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2. FUNCIONES\n",
    "#Function for handling IDEAM's CSV files of stations, both from the IDEAM entity and other entities.\n",
    "def CrearCarpetas(nombres_carpetas, ruta):\n",
    "    for nombre in nombres_carpetas:\n",
    "        ruta_completa = os.path.join(ruta, nombre)\n",
    "        if not os.path.exists(ruta_completa):\n",
    "            os.mkdir(ruta_completa)\n",
    "def get_most_common_date_from_files(folder_path):\n",
    "    all_dates = []\n",
    "\n",
    "    # Verificar si la carpeta está vacía\n",
    "    if not os.listdir(folder_path):\n",
    "        # Si la carpeta está vacía, retornar una fecha predeterminada\n",
    "        return datetime(1999, 1, 1)\n",
    "    \n",
    "    # Iterar sobre los archivos en la carpeta\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        if os.path.isfile(file_path) and file_name.endswith('.csv'):\n",
    "            # Leer el archivo CSV\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Asegurarse de que la columna 'fecha' está en formato datetime\n",
    "            df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "\n",
    "            # Obtener la última fila y su fecha\n",
    "            last_date = df['fecha'].iloc[-1]\n",
    "            all_dates.append(last_date)\n",
    "\n",
    "    # Calcular la fecha promedio más repetida\n",
    "    if all_dates:\n",
    "        most_common_date = Counter(all_dates).most_common(1)[0][0]\n",
    "        return most_common_date\n",
    "    else:\n",
    "        # Si no se encuentra ninguna fecha, retornar la fecha predeterminada\n",
    "        return datetime(1999, 1, 1)\n",
    "def get_max_date_from_files(folder_path):\n",
    "    all_dates = []\n",
    "\n",
    "    # Verificar si la carpeta está vacía\n",
    "    if not os.listdir(folder_path):\n",
    "        # Si la carpeta está vacía, retornar una fecha predeterminada\n",
    "        return datetime(1999, 1, 1)\n",
    "    \n",
    "    # Iterar sobre los archivos en la carpeta\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        if os.path.isfile(file_path) and file_name.endswith('.csv'):\n",
    "            # Leer el archivo CSV\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Asegurarse de que la columna 'fecha' está en formato datetime\n",
    "            df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "\n",
    "            # Obtener la última fila y su fecha\n",
    "            last_date = df['fecha'].iloc[-1]\n",
    "            all_dates.append(last_date)\n",
    "\n",
    "    # Calcular la fecha máxima\n",
    "    if all_dates:\n",
    "        max_date = max(all_dates)  # Encuentra la fecha máxima\n",
    "        return max_date\n",
    "    else:\n",
    "        # Si no se encuentra ninguna fecha, retornar la fecha predeterminada\n",
    "        return datetime(1999, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e825fcf-936b-4df3-ab54-25a1239bddd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3. INPUT INFORMATION\n",
    "# 3.1 Paths\n",
    "path_in1=config.path_in1_AUT\n",
    "# 3.2 Databases and connection parameters\n",
    "client = config.client\n",
    "# 3.3 Vectors\n",
    "# 0: Datasets that already have more than 95% loaded into the Alejandria database.\n",
    "dicc1_variables = config.dicc1_variables\n",
    "# 3.4 Inicial configuration\n",
    "ColumnsQueryOBS=['codigoestacion,fechaobservacion,valorobservado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a540574-c13d-4077-bee6-d8b316e202c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning the analysis of the variable: wind_direction\n",
      "Retrieved 0 records for 1999-01-01T00:00:00.000 to 1999-01-06T00:00:00.000\n",
      "Retrieved 0 records for 1999-01-06T00:00:00.000 to 1999-01-11T00:00:00.000\n",
      "Retrieved 0 records for 1999-01-11T00:00:00.000 to 1999-01-16T00:00:00.000\n",
      "\n",
      " ---> HTTPError: 500 Server Error: Server Error.\n",
      "\tInternal error: please include code 48a2d366-5f57-46e8-815a-fe66d4929c83 if you report the error. Waiting 20 seconds...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(client\u001b[38;5;241m.\u001b[39mget(value[\u001b[38;5;241m0\u001b[39m], where\u001b[38;5;241m=\u001b[39mQDate, select\u001b[38;5;241m=\u001b[39mColumnsQueryOBS, limit\u001b[38;5;241m=\u001b[39mlimit_columns))\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df1)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\coastsat\\Lib\\site-packages\\sodapy\\socrata.py:412\u001b[0m, in \u001b[0;36mSocrata.get\u001b[1;34m(self, dataset_identifier, content_type, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m params \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mclear_empty_values(params)\n\u001b[1;32m--> 412\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perform_request(\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, resource, headers\u001b[38;5;241m=\u001b[39mheaders, params\u001b[38;5;241m=\u001b[39mparams\n\u001b[0;32m    414\u001b[0m )\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\coastsat\\Lib\\site-packages\\sodapy\\socrata.py:555\u001b[0m, in \u001b[0;36mSocrata._perform_request\u001b[1;34m(self, request_type, resource, **kwargs)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m202\u001b[39m):\n\u001b[1;32m--> 555\u001b[0m     utils\u001b[38;5;241m.\u001b[39mraise_for_status(response)\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# when responses have no content body (ie. delete, set_permission),\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# simply return the whole response\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\coastsat\\Lib\\site-packages\\sodapy\\utils.py:30\u001b[0m, in \u001b[0;36mraise_for_status\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m     29\u001b[0m     http_error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(more_info)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mHTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39mresponse)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 500 Server Error: Server Error.\n\tInternal error: please include code 48a2d366-5f57-46e8-815a-fe66d4929c83 if you report the error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 83\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (ReadTimeout, \u001b[38;5;167;01mConnectionError\u001b[39;00m, HTTPError, ChunkedEncodingError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m ---> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Waiting 20 seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m20\u001b[39m)  \u001b[38;5;66;03m# Esperar 20 segundos antes de reintentar\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m#except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError, requests.exceptions.HTTPError) as e:\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m#    print(f\"\\n ---> {type(e).__name__}: {str(e)}. Waiting 20 seconds...\")\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m#    \u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m#    time.sleep(20)  # Wait 20 seconds before retrying\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Update start_date for the next iteration (move to the next time window)\u001b[39;00m\n\u001b[0;32m     89\u001b[0m start_date \u001b[38;5;241m=\u001b[39m end_date\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4. PROCESSES\n",
    "# 4.2 Step PP 2. For loop, Variables.\n",
    "# Get the current date\n",
    "current_date = datetime.now()\n",
    "for key, value in dicc1_variables.items():\n",
    "    print(f'Beginning the analysis of the variable: {value[2]}')  # Print the current analysis being processed\n",
    "    # Create folders\n",
    "    CrearCarpetas([value[2]], f'{path_in1}')\n",
    "    \n",
    "    # Initial configuration\n",
    "    if value[4]==0:start_date = get_most_common_date_from_files(f'{path_in1}/{value[2]}')\n",
    "    if value[4]==1:start_date =get_max_date_from_files(f'{path_in1}/{value[2]}')\n",
    "    limit_columns = 500000\n",
    "    days_to_add = 5  # Initially add 5 days\n",
    "    hours_to_add = 24  # Variable to subtract hours when days are 1\n",
    "\n",
    "    # Loop while end_date is less than or equal to the current date\n",
    "    while start_date <= current_date + timedelta(days=5):\n",
    "        # Format start_date and end_date\n",
    "        start_date_str = start_date.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]\n",
    "        end_date = start_date + timedelta(days=days_to_add)\n",
    "        end_date_str = end_date.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]\n",
    "        \n",
    "        # Get the data\n",
    "        consulta = {\n",
    "            \"select\": \"*\",  # Get all information\n",
    "        }\n",
    "        while True:\n",
    "            QDate = f\"((fechaobservacion >= '{start_date_str}') AND (fechaobservacion <= '{end_date_str}'))\"\n",
    "            try:\n",
    "                df1 = pd.DataFrame(client.get(value[0], where=QDate, select=ColumnsQueryOBS, limit=limit_columns))\n",
    "                print(f\"Retrieved {len(df1)} records for {start_date_str} to {end_date_str}\")\n",
    "                \n",
    "                # If the number of records equals the limit, reduce the days and retry the query\n",
    "                if len(df1) == limit_columns:\n",
    "                    if days_to_add > 1:\n",
    "                        # If we can still reduce the days, reduce the days by 1 (minimum 1 day)\n",
    "                        days_to_add = max(1, days_to_add - 1)\n",
    "                        # If no more days can be reduced, start subtracting hours\n",
    "                        end_date = start_date + timedelta(days=days_to_add)\n",
    "                        end_date_str = end_date.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]\n",
    "                        \n",
    "                    else:\n",
    "                        # Start subtracting hours\n",
    "                        hours_to_add = max(1, hours_to_add - 1)\n",
    "                        end_date = start_date + timedelta(hours=hours_to_add)\n",
    "                        end_date_str = end_date.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]\n",
    "                    continue  # Retry the same time window with fewer days or hours\n",
    "                if len(df1)==0: break\n",
    "                # Processing of dataframe\n",
    "                grouped = df1.groupby('codigoestacion')\n",
    "                \n",
    "                for station_code, group in grouped:\n",
    "                    # Prepare the dataframe with date and observed value columns\n",
    "                    df2 = group[['fechaobservacion', 'valorobservado']].reset_index(drop=True)\n",
    "                    df2.columns = ['fecha', 'valor']\n",
    "                    \n",
    "                    # Convert 'fechaobservacion' to datetime\n",
    "                    df2['fecha'] = pd.to_datetime(df2['fecha'])\n",
    "                    \n",
    "                    # Define the full path where the file will be saved\n",
    "                    file_path = f'{path_in1}/{value[2]}/{station_code}.csv'\n",
    "                    \n",
    "                    # Check if the file already exists\n",
    "                    if os.path.exists(file_path):\n",
    "                        # Load the existing file\n",
    "                        existing_df = pd.read_csv(file_path)\n",
    "                        \n",
    "                        # Convert the 'fechaobservacion' column from the existing file to datetime (if it's not already)\n",
    "                        existing_df['fecha'] = pd.to_datetime(existing_df['fecha'])\n",
    "                        \n",
    "                        # Merge, removing duplicate dates and keeping the first one\n",
    "                        df2 = pd.concat([existing_df, df2]).drop_duplicates(subset=['fecha'], keep='first').sort_values('fecha')\n",
    "                        df2.reset_index(drop=True, inplace=True)  # Reset the index after the merge\n",
    "                    \n",
    "                    # Sort the dates from most recent to least before saving\n",
    "                    df2 = df2.sort_values('fecha', ascending=False)\n",
    "                    #df2 = df2.sort_values(by='fecha', ascending=False)\n",
    "                    df2.to_csv(file_path, index=False, encoding='utf-8-sig')  # Save the updated file (new or merged)\n",
    "                break  # Exit the loop when valid data is retrieved (less than the limit_columns)\n",
    "            except (ReadTimeout, ConnectionError, HTTPError, ChunkedEncodingError) as e:\n",
    "                print(f\"\\n ---> {type(e).__name__}: {str(e)}. Waiting 20 seconds...\")\n",
    "                time.sleep(20)  # Esperar 20 segundos antes de reintentar\n",
    "            #except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError, requests.exceptions.HTTPError) as e:\n",
    "            #    print(f\"\\n ---> {type(e).__name__}: {str(e)}. Waiting 20 seconds...\")\n",
    "            #    \n",
    "            #    time.sleep(20)  # Wait 20 seconds before retrying\n",
    "        # Update start_date for the next iteration (move to the next time window)\n",
    "        start_date = end_date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
